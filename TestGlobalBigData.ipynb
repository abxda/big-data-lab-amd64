{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc17db-739f-4684-b13b-489d56c295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Celda 1 (Versi√≥n Definitiva para MODO LOCAL) ---\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Detiene cualquier sesi√≥n previa\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Construye la sesi√≥n en MODO LOCAL. No necesita el conector nativo.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnalisisFlotaDrones_Local\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Cliente para Elasticsearch (para borrar el √≠ndice despu√©s)\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Verificar conexiones\n",
    "if es_client.ping():\n",
    "    print(\"‚úÖ Conexi√≥n con Elasticsearch exitosa.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: No se pudo conectar a Elasticsearch.\")\n",
    "\n",
    "print(\"‚úÖ Sesi√≥n de Spark y clientes listos.\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0b903-fb0e-4fab-bf2f-58caabdcecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225d9ed-6465-4f26-947f-8ab32d499293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Celda 2 Mejorada: Generar o Cargar Datos de Drones ---\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_filename = 'drone_sensors_data.csv'\n",
    "\n",
    "# Comprobar si el archivo ya existe en la carpeta local\n",
    "if not os.path.exists(csv_filename):\n",
    "    print(f\"El archivo '{csv_filename}' no existe. Generando nuevos datos...\")\n",
    "    \n",
    "    # --- Generar Datos de Drones con Pandas ---\n",
    "    num_drones = 50\n",
    "    data = {\n",
    "        'drone_id': [f'DRN-{i:03}' for i in range(1, num_drones + 1)],\n",
    "        'bateria_restante': [round(20 + 80 * os.urandom(1)[0] / 255, 2) for _ in range(num_drones)],\n",
    "        'temperatura_motor': [round(60 + 40 * os.urandom(1)[0] / 255, 2) for _ in range(num_drones)],\n",
    "        'vibracion_hz': [round(5 + 25 * os.urandom(1)[0] / 255, 2) for _ in range(num_drones)]\n",
    "    }\n",
    "    df_pandas = pd.DataFrame(data)\n",
    "\n",
    "    # Guardar localmente en la carpeta de notebooks\n",
    "    df_pandas.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Archivo '{csv_filename}' creado con {len(df_pandas)} registros.\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚úÖ El archivo '{csv_filename}' ya existe. Cargando datos desde el archivo.\")\n",
    "    # Cargar los datos desde el CSV existente\n",
    "    df_pandas = pd.read_csv(csv_filename)\n",
    "    print(f\"Cargados {len(df_pandas)} registros.\")\n",
    "\n",
    "# Mostrar las primeras 5 filas para verificar\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd7f09-2f6d-418a-984d-fff5fc5aed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Celda 3 Mejorada: Subir a HDFS (si es necesario) ---\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError\n",
    "\n",
    "# Cliente para interactuar con HDFS\n",
    "hdfs_client = InsecureClient('http://localhost:9870')\n",
    "\n",
    "# Definir rutas\n",
    "hdfs_path_raw = '/data/raw/drones'\n",
    "hdfs_filepath = f'{hdfs_path_raw}/{csv_filename}'\n",
    "\n",
    "try:\n",
    "    # Intenta obtener el estado del archivo. Si no existe, lanzar√° una HdfsError.\n",
    "    status = hdfs_client.status(hdfs_filepath)\n",
    "    print(f\"‚úÖ El archivo ya existe en HDFS en '{hdfs_filepath}'. No se necesita subir de nuevo.\")\n",
    "    \n",
    "except HdfsError:\n",
    "    # Si el archivo no existe, la excepci√≥n HdfsError es capturada.\n",
    "    print(f\"El archivo no existe en HDFS. Procediendo a la subida...\")\n",
    "    \n",
    "    # Asegurarse de que el directorio base exista.\n",
    "    hdfs_client.makedirs(hdfs_path_raw)\n",
    "    print(f\"Directorio '{hdfs_path_raw}' verificado/creado en HDFS.\")\n",
    "    \n",
    "    # Subir el archivo, overwrite=True es seguro aqu√≠ porque ya sabemos que no existe,\n",
    "    # pero es una buena pr√°ctica por si ocurre algo entre la comprobaci√≥n y la subida.\n",
    "    hdfs_client.upload(hdfs_path_raw, csv_filename, overwrite=True)\n",
    "    \n",
    "    print(f\"‚úÖ Archivo '{csv_filename}' subido exitosamente a HDFS en: '{hdfs_filepath}'\")\n",
    "\n",
    "finally:\n",
    "    # En cualquier caso (exista o no), listar el contenido para confirmar.\n",
    "    print(\"\\nContenido actual en HDFS en el directorio /data/raw/drones:\")\n",
    "    print(hdfs_client.list(hdfs_path_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd5108-3a86-407d-a324-ad4022a848a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Leer desde HDFS y Procesar con Spark ---\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_spark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"hdfs://localhost:9000{hdfs_filepath}\")\n",
    "\n",
    "print(\"Esquema inferido por Spark:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# Calcular un \"√çndice de Riesgo\"\n",
    "# El riesgo aumenta si la bater√≠a es baja, la temperatura es alta o la vibraci√≥n es alta\n",
    "df_analizado = df_spark.withColumn(\n",
    "    \"indice_riesgo\",\n",
    "    (\n",
    "        when(col(\"bateria_restante\") < 30, 1).otherwise(0) +\n",
    "        when(col(\"temperatura_motor\") > 85, 1).otherwise(0) +\n",
    "        when(col(\"vibracion_hz\") > 20, 1).otherwise(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame con √çndice de Riesgo calculado:\")\n",
    "df_analizado.show()\n",
    "\n",
    "# Filtrar solo los drones que necesitan mantenimiento (riesgo > 0)\n",
    "drones_en_riesgo = df_analizado.filter(col(\"indice_riesgo\") > 0).sort(col(\"indice_riesgo\").desc())\n",
    "\n",
    "print(\"\\nüö® Drones que requieren atenci√≥n inmediata:\")\n",
    "drones_en_riesgo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431ced7-0b23-465a-82ff-74f5ede9e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Celda 5 (Tu Soluci√≥n): Convertir el resultado final a Pandas y Cargar ---\n",
    "\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Re-creamos el cliente por si la sesi√≥n se reinici√≥\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "es_index_name = \"drones_en_riesgo\"\n",
    "\n",
    "print(f\"Preparando para enviar los resultados a Elasticsearch...\")\n",
    "\n",
    "# Borrar el √≠ndice si ya existe para una prueba limpia\n",
    "if es_client.indices.exists(index=es_index_name):\n",
    "    es_client.indices.delete(index=es_index_name)\n",
    "    print(f\"√çndice '{es_index_name}' antiguo borrado.\")\n",
    "\n",
    "# 1. PASO CLAVE: Convertir el DataFrame final de Spark a un DataFrame de Pandas\n",
    "# Esta es la √∫nica acci√≥n que trae datos del entorno Spark al entorno Python.\n",
    "print(\"Convirtiendo resultado de Spark ('drones_en_riesgo') a Pandas...\")\n",
    "df_pandas_final = drones_en_riesgo.toPandas()\n",
    "print(f\"Conversi√≥n completa. Se van a indexar {len(df_pandas_final)} drones.\")\n",
    "\n",
    "# 2. Convertir el DataFrame de Pandas a una lista de diccionarios\n",
    "# (Este formato es ideal para el cliente de Elasticsearch)\n",
    "documentos_para_es = df_pandas_final.to_dict(orient='records')\n",
    "\n",
    "# 3. Indexar la lista de drones en Elasticsearch\n",
    "print(\"Indexando documentos en Elasticsearch...\")\n",
    "for doc in documentos_para_es:\n",
    "    # Usamos el cliente de python que ya sabemos que funciona\n",
    "    es_client.index(index=es_index_name, document=doc, id=doc['drone_id'])\n",
    "\n",
    "# 4. Refrescar el √≠ndice para que los datos est√©n disponibles para b√∫squeda\n",
    "es_client.indices.refresh(index=es_index_name)\n",
    "\n",
    "print(f\"\\n‚úÖ ¬°√âXITO! Datos indexados en Elasticsearch exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3df638-a6e0-4894-aadb-65d37ddfbe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e7210-c631-4f31-91f5-adc94eb9cc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
